#! /usr/bin/env ruby
#   TODO
#
# 3. The ability to control output "select rhs.field lhs.field"
# 4. More efficient disk storage systems (+my/sql/ite)
# 5. Threading on disk reads
# 6. ETAs and better output
#



#
# Large CSV merging app.  uses an index construction method to merge CSVs in excess of memory size
require 'csv'


LOW_MEMORY      = false     # Keep memory usage to an absolute minimum (not recommended unless you have almost no RAM)
STRICT_GC       = true      # Garbage collect after large operations (recommended) 
OUTPUT_DIVISOR  = 512       # print the line every n lines.  Should be a largeish and fast-to-divide number, ie a power of 2





module CacheMethods
  class Cache
    # Add an item to the cache
    #
    # Key will be the CSV minimal key
    # offset will be the file offset, int.
    def []=(key, offset)
      puts "STUB: []= in CacheMethods::Cache"
    end

    # Retrieve a file offset using a key
    #
    # key will be the CSV minimal key
    def [](key)
      puts "STUB: [] in CacheMethods::Cache"
    end

    # Signal that we no longer wish to make any 
    # changes
    #
    # This allows the cache to optimise itself, or
    # switch to read only mode.
    def finalise
    end

    # Signal that we no longer wish to use the store
    # This allows things to remove their disk stuff
    # and/or clean up RAM
    def cleanup
    end
  end

  # Holds everything in an in-memory hash for speed
  class MemoryCache < Cache
    def initialize
      @cache = {}
    end

    def []=(key, offset)
      @cache[key] = offset
    end

    def [](key)
      @cache[key]
    end
  end

  # Holds everything in PStore.
  # VERY SLOW, but works.
  class DiskCache < Cache
    def initialize(filename="/tmp/csvmerge.tmp", transaction_size=10000)
      require 'pstore'
      @cache            = PStore.new(filename, false)
      @read_only        = false
      @pending          = {}
      @transaction_size = transaction_size.to_i
    end

    def []=(key, offset)
      raise "Cannot store!" if @read_only

      # Write into the memory cache-cache
      @pending[key] = offset

      # If we hit the transaction count, write
      write_to_disk if @pending.size >= @transaction_size
    end

    def [](key)
      @cache.transaction(true){|cache|
        return cache[key]
      }
    end

    def finalise
      write_to_disk
      @read_only = true
    end

    def cleanup
      File.rm(@cache.path)
    end

  private
    def write_to_disk
      @cache.transaction(false){|cache|
        @pending.each{|k, v|
          cache[k] = v
        }
      }
    end
  end
end

# Provides a wrapper around CSV
# that will perform various checks and descriptive statistical ops.
class KeyValueCSV 

  attr_reader :keys, :filename

  def initialize(filename, keys)
    @filename = filename
  
    # Load keys
    @keys = (keys.empty?) ? fields : keys
    # Check all the keys are there
    @keys.each{|k|
      raise "Key is not in fields list" if not fields.include?(k)
    }
  end

  def say(arg)
    puts "[#{File.basename(@filename)}] #{arg}"
  end

  def minimal_keys
    compute_minimal_keys if not @minimal_keys
    @minimal_keys
  end

  # Total size of the key
  def key_size
    compute_minimal_keys if not @key_length
    @key_length 
  end

  # Retrieve a key from a line
  def get_minimal_key(line)
    compute_minimal_keys if not @minimal_keys
   
    str = ""
    minimal_keys.each{|k, size|
      str += line.field(k).to_s[0..size]
    }
    return str
  end

  def fields
    build_fields if not @fields
    @fields
  end

  def count
    build_count if not @count
    @count
  end

  def to_s
    return "#{@filename}"
  end

  def cache=(cache)
    raise "Cannot set cache when seeking" if @handle
    @cache = cache
    build_index
  end
 
  def seek_retrieval(&block)
    File.open(@filename, 'r') do |handle|
      @handle = handle
      yield
      @handle = nil
    end
  end

  def []=(k, v)
    raise "No Cache!" if not @cache
    @cache[k] = v
  end

  def [](k)
    raise "Cannot retrieve from cache if not seeking" if not @handle
    raise "No Cache" if not @cache

    # Seek to the location in the cache and return a line
    if seek_to = @cache[k] then
      @handle.seek(seek_to)
      return CSV.parse_line(@handle.readline, :encoding => 'utf-8', :headers => fields).to_hash
    else
      return nil
    end
  end

private
    
  
  # Construct an index.  Should not need to be
  # overidden
  def build_index
    say "Building index..."

    # Get size in bytes, so we know when we've hit the end.
    file_size = File.size(@filename)
    CSV.open(@filename, :encoding => 'utf-8', :headers => true) do |csvin|

      # Get byte offset
      line_start = csvin.tell

      # Then read line
      count = 0
      while((line_start = csvin.tell) < file_size) do

        # Load the line
        line = csvin.shift()

        # Load the key up to the key size only
        key = get_minimal_key(line)
        
        # Save the file offset
        # TODO: ensure random access of the cache is possible
        $stderr.puts "WARNING: Key at byte #{line_start} of #{@filename} collides with key at byte #{@cache[key]}." if @cache[key]
        @cache[key] = line_start

        print "\rLine: #{count+=1}  "
      end
    end
    print "\n"
    
    say "Finished building index"
  end
  
  
  def compute_minimal_keys
    @minimal_keys = {}


    say "Calculating minimum index size (0/2)..."

    # Set up per-key prefix table and max length measure
    prefix_tables = {}
    max_lengths   = {}
    @keys.each{ |k| 
      max_lengths[k]    = 0   # length of the field
      prefix_tables[k]  = []  # position-usage measure
    }

    # Enable garbage collector stress mode, to keep memory clean
    GC.stress = true if LOW_MEMORY 

    count = 0
    CSV.foreach(@filename, :encoding => 'utf-8', :headers => true) do |csvin|
      prefix_tables.each{ |key, prefix|
        val = csvin.field(key).to_s

        # For each position, for each char, keep a count.
        pos = 0
        val.each_char{ |c|
          prefix[pos]     = {} if not prefix[pos]
          prefix[pos][c]  = 0  if not prefix[pos][c]
          prefix[pos][c] += 1
          pos            += 1
        }

        # Check on the maximum length for this field
        max_lengths[key] = pos if max_lengths[key] < pos
      }

      # count records
      print "\rLine: #{count} " if (count+=1) % OUTPUT_DIVISOR == 0
    end
    print "\rLine: #{count} " # ensure we print the last number to avoid confusion
    print "\n"

    # OPTIMISATION:
    # completes file size count if not done already
    @count = count if not @count 
    say "Prefix tables complete (1/2)"


    # And turn it off again
    GC.stress = false  if LOW_MEMORY 
    GC.start           if STRICT_GC

    say "Computing minimum size (2/2)..."
    # For each key, compute the minimal size from the prefix table
    count = 0
    prefix_tables.each{|key, prefix|
      print "\rField: #{count+=1}/#{prefix_tables.size}"

      # From the hash of prefixes, determine the shortest possible prefix
      key_size = 0
      prefix.each{|chars|
        key_size  += 1
        max        = 1
        # puts "prefix for field #{key}, filename #{@filename}: position #{key_size} sees #{prefix[key_size-1].size} #{chars.size} different chars."
        chars.each{|k, v|  max = v if v > max  }
        break if max == 1 # Each char is at this point used only once
      }
      throw "No key values for field #{key} in #{@filename}" if key_size == 0

      # If the final character position has only seen one character
      # then this field is NOT unique
      # non_unique_fields << key if prefix[key_size-1].length == 1
      # FIXME
      # if prefix[key_size-1].length == 1 then
      # end
       
      # FIXME
      # puts "\nWARNING: field '#{key}' in #{@filename} is uninformative, with an entropy of 0." if key_size == 0

      # Write the minimal key size for this key
      @minimal_keys[key] = key_size 
    }
    print "\n"

    say "Minimum index size established."

    # Lastly, compute total key length
    @key_length = minimal_keys.values.inject(0, :+)
  end

  def build_count
    say "Counting items..."
    count = 0
 
    # Enable garbage collector stress mode, to keep memory clean
    GC.stress = true if LOW_MEMORY 

    count = CSV.read(@filename,  :encoding => 'utf-8', :headers => true).length
    # CSV.foreach(@filename, {:headers => true}) do |csvin|
    #   count += 1
    # end

    # And turn it off again
    GC.stress = false  if LOW_MEMORY 
    GC.start           if STRICT_GC

    say "Count complete."
    @count = count
  end

  def build_fields
    say "Building field list..."

    csv = CSV.open(@filename, 'r', :encoding => 'utf-8', :headers => true )
    # Shift once to get past the header, if it exists
    csv.shift()
    row = csv.shift()

    # Then list headers and close
    list = row.headers
    csv.close

    say "Field list complete."

    # Ensure they're strings
    @fields = list.map!{|x| x.to_s }
  end

end

module MergePolicy
  class Merge
    def initialize(lhs, rhs)
      @lhs, @rhs  = lhs, rhs
    end

    # Output fields as they will be, i.e. for the header.
    # Prefix them with rhs. if they would otherwise be dupes.
    def fields
      @lhs.fields + (@rhs.fields - @rhs.keys).map{|f| 
        @lhs.fields.include?(f) ? "rhs.#{f}" : f
      }
    end

    def to_s
      "Merge"
    end

    def accept_line?(lhs_line, rhs_line)
      puts "WARNING: No merge policy."
      return false
    end

    # Merge a single line
    def merge_line(lhs_line, rhs_line)
      line = []

      # Add all left hand side items
      @lhs.fields.each{ |f| line << lhs_line[f] }
      # and only non-key fields from the RHS
      (@rhs.fields - @rhs.keys).each{|f| 
        line << rhs_line[f] 
      }

      return line
    end
  end


  class LeftMerge < Merge
    def to_s
      "Left Merge"
    end

    def accept_line?(lhs_line, rhs_line)
      # Left outer joins returns ALL from LHS, even if RHS is empty
      true
    end
  end
  
  class InnerMerge < Merge
    def to_s
      "Inner Merge"
    end

    def accept_line?(lhs_line, rhs_line)
      # Inner join returns rows ONLY if both have data
      (not lhs_line.empty?) and (not rhs_line.empty?)
    end
  end
 
  # TODO
  #
  # An outer join will output even if
  # one half of the key is missing,
  # but match up what it can
#  class OuterMerge < Merge
#    def to_s
#      "Outer Merge"
#    end
#
#    # Accept ALL, in fact, generate some.
#    # currently impossible using this algorithm
#    def accept_line?(lhs_line, rhs_line)
#    end
#  end
 
  # TODO:
  # 
  # A cross join is the cartesian product of
  # both files.
  #
  # Currently not possible with the framework as it is
#  class CrossMerge < Merge
#    def to_s
#      "Custom Merge"
#    end
#    def accept_line?(lhs_line, rhs_line)
#    end
#  end
end


def process_commandline_args
  # Parse command line args
  if ARGV.length < 5 then
    $stderr.puts "USAGE: #{__FILE__} LHS RHS OUT MERGE_POLICY CACHE lkey1 lkey2 lkey3... -- rkey1 rkey2 rkey3..."
    exit(1)
  end

  # Load the basics
  lhs     = ARGV[0]
  rhs     = ARGV[1]
  out     = ARGV[2]
  policy  = ARGV[3]
  cache   = ARGV[4]
  lhs_keys = []
  rhs_keys = []

  if not %w{LeftMerge InnerMerge}.include?(policy) then
    $stderr.puts "No such merge policy: '#{policy}'"
    exit(1)
  end

  if not File.exist?(lhs)
    $stderr.puts "File does not exist: #{lhs}"
    exit(1)
  end

  if not File.exist?(rhs)
    $stderr.puts "File does not exist: #{rhs}"
    exit(1)
  end

  if File.exist?(out)
    $stderr.puts "WARNING: Output file exists."
  end

  if not %w{MemoryCache DiskCache} then
    $stderr.puts "No such cache: '#{cache}'"
    exit(1)
  end

  # Load keys into the rhs, lhs key lists
  arr = lhs_keys
  ARGV[5..-1].each{|arg|
    if arg == "--" then
      arr = rhs_keys
    else
      arr << arg
    end
  }

  if lhs_keys.empty? then
    $stderr.puts "No keys given for LHS: using all fields as key."
  end

  if lhs_keys.empty? then
    $stderr.puts "No keys given for RHS: using all fields as key."
  end

  return {:lhs      => lhs,
          :rhs      => rhs,
          :output   => out,
          :policy   => policy,
          :cache    => cache,
          :lhs_keys => lhs_keys,
          :rhs_keys => rhs_keys
         }
end

# Load config
config = process_commandline_args 

# TODO: support compound keys from the command line
lhs     = KeyValueCSV.new(config[:lhs], config[:lhs_keys])
rhs     = KeyValueCSV.new(config[:rhs], config[:rhs_keys])
merge   = eval("MergePolicy::#{config[:policy]}.new(lhs, rhs)")


puts "Merging #{lhs} into #{rhs} with policy '#{merge}'"
puts ""
puts "Fields (*keys):"
puts "   LHS: #{lhs.fields.map{|f| lhs.keys.include?(f) ? '*'+f : f}.join(', ')}"
puts "   RHS: #{rhs.fields.map{|f| rhs.keys.include?(f) ? '*'+f : f}.join(', ')}"
puts "Output: #{merge.fields.join(', ')}"
puts ""


# puts "\nCounting records in each (and validating CSV)"
# puts "   LHS: #{lhs.count}"
# puts "   RHS: #{rhs.count}"
# 
# $stderr.puts "WARNING: RHS file is much larger.  Consider reversing the order of files if you are performing a symmetric join." if rhs.count > lhs.count
# 
# puts "Finding Minimal Key Lengths for #{rhs}..."
# rhs.minimal_keys.each{|k, v|
#   puts " KEY #{rhs}: #{k} => #{v}"
# }
# 
# 
# puts "Key size is #{rhs.key_size}B, with #{rhs.count} records this means #{(((rhs.key_size + [rhs.count].pack('l').size)*rhs.count)/1024/1024).round(2)}MB needed for index."

rhs.cache = eval("CacheMethods::#{config[:cache]}.new")


############## Perform Merge #################################
count = 0
CSV.open(config[:output], 'w') do |out_csv|
  out_csv << merge.fields

  
  puts "Building output CSV..."

  # Open a transaction for the rhs file,
  # in readiness for reading from keys
  rhs.seek_retrieval{

    # The one we don't have the index for
    CSV.foreach(lhs.filename, :encoding => 'utf-8', :headers => true) do |lhs_row|

      # Generate a key from the row
      key         = rhs.get_minimal_key(lhs_row)
    
      # Look up the value from the LHS cache
      # This uses file seeking, so is quickish
      # and low on memory
      rhs_vals    = rhs[key] || {}

      # Ensure the RHS vals are in hash form 
      lhs_vals    = lhs_row.to_hash

      # Merge according to policy and output
      out_csv << merge.merge_line(lhs_vals, rhs_vals) if merge.accept_line?(lhs_vals, rhs_vals)

      print "\rLine: #{count} " if (count+=1) % OUTPUT_DIVISOR == 0
    end
  }
end
print "\rLine: #{count} " # Ensure we always print the last line number for clarity
print "\n"

# End
puts "Done."
exit
